(…) NLP QA and concept understanding in general (language) as completions of paths in knowledge graphs (ie wiki, own thoughts)



(…) NN to understand workings of others NNs (yield good updates — formulate through RL objective, semantic understanding across domains and universal function understanding to synthesize into other concepts)



(…) RL as more “black box opt”, more power comes from knowing more inner workings 



(…) NN more in charge of its own workings (hyperparams etc) example of looking at other neural network’s structure through “convnet”-esque filter, let NN choose own filter sizes iteratively in process of exploration for the sake of truly understanding the nature of the current data (let’s say starting with 3 attempts of increasing granularity) à la sequence model and train via RL. Can overcome overfitting/bias problems perhaps? 

